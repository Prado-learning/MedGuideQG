# import torch
# from transformers import AutoModelForCausalLM, AutoTokenizer

# # æ¨¡å‹åç§°
# model_name = "Qwen/Qwen2.5-7B-Instruct"

# # åŠ è½½ tokenizer
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# # åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒ 8-bit é‡åŒ–å‡å°‘æ˜¾å­˜å ç”¨
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype="auto",
#     device_map="auto"
# )

# # è®¾å¤‡é€‰æ‹©ï¼ˆGPU ä¼˜å…ˆï¼‰
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# model.to(DEVICE)

# def chat_with_model(user_input):
#     """ æ ¹æ®ç”¨æˆ·è¾“å…¥ç”ŸæˆåŒ»ç–—é—®é¢˜çš„å›ç­” """
#     messages = [
#         {"role": "system", "content": "You are Qwen, a medical assistant. Provide helpful and accurate medical information."},
#         {"role": "user", "content": user_input},
#     ]
    
#     # æ ¼å¼åŒ–ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼
#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

#     # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
#     model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

#     # ç”Ÿæˆå›ç­”
#     with torch.no_grad():
#         generated_ids = model.generate(
#             **model_inputs,
#             max_new_tokens=256,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
#         )
    
#     # ä»…æå–ç”Ÿæˆçš„å†…å®¹
#     generated_ids = [
#         output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
#     ]

#     # è§£ç æ¨¡å‹è¾“å‡º
#     response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
#     return response

# # äº¤äº’å¼é—®ç­”å¾ªç¯
# if __name__ == "__main__":
#     print("ğŸ’¡ åŒ»ç–—é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ï¼Œè¾“å…¥é—®é¢˜ä»¥è·å–ç­”æ¡ˆï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰")
#     while True:
#         user_input = input("ğŸ©º ä½ : ")  # ç”¨æˆ·è¾“å…¥
#         if user_input.lower() in ["exit", "quit", "é€€å‡º"]:
#             print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
#             break
        
#         response = chat_with_model(user_input)
#         print(f"ğŸ¤– AI: {response}\n")
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# æ¨¡å‹åç§°
model_name = "Qwen/Qwen2.5-7B-Instruct"

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

# è®¾å¤‡é€‰æ‹©
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
model.to(DEVICE)

def generate_response(messages, max_tokens=512):
    """ ç”Ÿæˆæ¨¡å‹å›ç­” """
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=max_tokens
        )
    
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

def interactive_medical_qna():
    """ äº¤äº’å¼ä¸‰é˜¶æ®µåŒ»ç–—é—®ç­” """
    
    print("\nğŸ’¡ æ¬¢è¿ä½¿ç”¨æ™ºèƒ½åŒ»ç–—é—®ç­”ç³»ç»Ÿï¼Œè¯·æŒ‰æç¤ºè¾“å…¥ä¿¡æ¯ã€‚è¾“å…¥ 'exit' é€€å‡ºã€‚\n")

    # **ç¬¬ä¸€è½®: ç”¨æˆ·è¾“å…¥ç—…æƒ…æè¿°**
    patient_input = input("ğŸ©º è¯·æè¿°æ‚¨çš„ç—…æƒ…: ")
    if patient_input.lower() == "exit":
        print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
        return
    
    # **ç¬¬ä¸€è½®: è¯Šæ–­ & é‰´åˆ«è¯Šæ–­**
    messages_round1 = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
        {"role": "user", "content": f"ä½ å°†æ ¹æ®æ‚£è€…åœ¨{patient_input}ä¸­æä¾›çš„ä¿¡æ¯ï¼Œä»”ç»†æ€è€ƒï¼Œç»™å‡º1ä¸ªå¯èƒ½çš„è¯Šæ–­ã€‚\n"
                                    "éšåæ ¹æ®å¯èƒ½æ€§ä»é«˜åˆ°ä½ï¼Œä¾æ¬¡ç»™å‡º3è‡³5ä¸ªå¯èƒ½çš„é‰´åˆ«è¯Šæ–­ã€‚\n"
                                    "è¯·åªå›ç­”è¯Šæ–­å’Œé‰´åˆ«è¯Šæ–­ï¼Œä¸è¦åœ¨å›ç­”ä¸­åŒ…å«å…¶ä»–å†…å®¹ã€‚"}
    ]
    diagnosis_response = generate_response(messages_round1)
    print("\nğŸ”¹ **ç¬¬ä¸€è½® (è¯Šæ–­ & é‰´åˆ«è¯Šæ–­):**\n", diagnosis_response)

    # **ç¬¬äºŒè½®: ç”¨æˆ·è¾“å…¥äºŒè½®é—®é¢˜**
    print("\nğŸ’¡ ç°åœ¨ï¼Œè¯·åŸºäºä¸Šé¢çš„è¯Šæ–­ï¼Œè¾“å…¥æ‚¨æƒ³è¿›ä¸€æ­¥å’¨è¯¢çš„é—®é¢˜ã€‚")
    second_input = input("âœï¸ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
    if second_input.lower() == "exit":
        print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
        return
    
    # **ç¬¬äºŒè½®: è¯Šç–—æŒ‡å— & æ²»ç–—æ–¹æ¡ˆ**
    messages_round2 = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
        {"role": "user", "content": f"æ ¹æ®ä¸Šè¿°å›ç­”ï¼Œä½ è€ƒè™‘æ ¹æ®è¿™ä½æ‚£è€…{patient_input}æè¿°çš„å†…å®¹ï¼Œè€ƒè™‘æ‚£è€…çš„è¯Šæ–­æ˜¯{diagnosis_response}ã€‚\n"
                                    "è¿™æ˜¯è¿™ä¸ªç–¾ç—…çš„è¯Šç–—æŒ‡å—{æŒ‡å—}ï¼Œä½ å¿…é¡»æ ¹æ®æŒ‡å—ï¼Œå»å›ç­”ä»¥ä¸‹å†…å®¹:\n"
                                    "1. æ‚£è€…ç—…æƒ…çš„åˆ†æ\n"
                                    "2. æœ€ç»ˆè€ƒè™‘çš„è¯Šæ–­\n"
                                    "3. è¿›ä¸€æ­¥æ£€æŸ¥åŠæ²»ç–—æ–¹æ¡ˆ\n"
                                    "åœ¨å›ç­”è¿‡ç¨‹ä¸­ï¼Œè¯·å¼•ç”¨è¯Šç–—æŒ‡å—çš„åŸæ–‡ã€‚"}
    ]
    treatment_response = generate_response(messages_round2)
    print("\nğŸ”¹ **ç¬¬äºŒè½® (ç—…æƒ…åˆ†æ & æ²»ç–—æ–¹æ¡ˆ):**\n", treatment_response)
    
    # **ç¬¬ä¸‰è½®: ç”¨æˆ·è¾“å…¥ä¸‰è½®é—®é¢˜**
    print("\nğŸ’¡ ç°åœ¨ï¼Œè¯·è¾“å…¥æ‚¨çš„æœ€ç»ˆå’¨è¯¢é—®é¢˜ï¼Œè®©åŒ»ç”Ÿç»™æ‚¨ä¸€ä¸ªå®Œæ•´çš„æ€»ç»“ã€‚")
    third_input = input("âœï¸ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ")
    if third_input.lower() == "exit":
        print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
        return

    # **ç¬¬ä¸‰è½®: åŒ»ç”Ÿè§’è‰²æ€»ç»“ & è¯¦ç»†å›ç­”**
    messages_round3 = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
        {"role": "user", "content": f"æ ¹æ®ä¸Šè¿°å›ç­”çš„å†…å®¹ï¼Œè¿›è¡Œæ€»ç»“ï¼Œå¹¶ä»¥åŒ»ç”Ÿçš„è§’è‰²è¯¦ç»†çš„å›ç­”æ‚£è€…ã€‚\n"
                                    "è¯·ä½¿ç”¨ä¸“ä¸šçš„åŒ»å­¦ç”¨è¯­ï¼Œç¡®ä¿æ‚£è€…å¯ä»¥ç†è§£ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ²»ç–—å»ºè®®ã€‚"}
    ]
    final_response = generate_response(messages_round3)
    print("\nğŸ”¹ **ç¬¬ä¸‰è½® (åŒ»ç”Ÿè¯¦ç»†å›ç­”æ‚£è€…):**\n", final_response)

if __name__ == "__main__":
    interactive_medical_qna()

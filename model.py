# import torch
# from transformers import AutoModelForCausalLM, AutoTokenizer

# # æ¨¡å‹åç§°
# model_name = "Qwen/Qwen2.5-7B-Instruct"

# # åŠ è½½ tokenizer å’Œæ¨¡å‹
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype="auto",
#     device_map="auto"
# )

# # è®¾å¤‡é€‰æ‹©
# DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
# model.to(DEVICE)

# def generate_response(messages, max_tokens=512):
#     """ ç”Ÿæˆæ¨¡å‹å›ç­” """
#     text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
#     model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

#     with torch.no_grad():
#         generated_ids = model.generate(
#             **model_inputs,
#             max_new_tokens=max_tokens
#         )
    
#     generated_ids = [
#         output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
#     ]

#     return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

# def three_stage_medical_qna():
#     """ äº¤äº’å¼ä¸‰é˜¶æ®µåŒ»ç–—é—®ç­” """
    
#     print("\nğŸ©º **æ‚¨å¥½ï¼Œè¿™é‡Œæ˜¯æ™ºæ…§åŒ»ç–—å°åŠ©æ‰‹ï¼Œè¯·æ‚¨æè¿°æ‚¨çš„ç—…æƒ…ã€‚**\n")
    
#     # **ç¬¬ä¸€è½®: ç”¨æˆ·è¾“å…¥ç—…æƒ…æè¿°**
#     patient_input = input("ğŸ“ æ‚£è€…: ")
#     if patient_input.lower() == "exit":
#         print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
#         return
    
#     # **ç¬¬ä¸€è½®: è¯Šæ–­ & é‰´åˆ«è¯Šæ–­**
#     messages_round1 = [
#         {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
#         {"role": "user", "content": f"ä½ å°†æ ¹æ®æ‚£è€…åœ¨{patient_input}ä¸­æä¾›çš„ä¿¡æ¯ï¼Œä»”ç»†æ€è€ƒï¼Œç»™å‡º1ä¸ªå¯èƒ½çš„è¯Šæ–­ã€‚\n"
#                                     "éšåæ ¹æ®å¯èƒ½æ€§ä»é«˜åˆ°ä½ï¼Œä¾æ¬¡ç»™å‡º3è‡³5ä¸ªå¯èƒ½çš„é‰´åˆ«è¯Šæ–­ã€‚\n"
#                                     "è¯·åªå›ç­”è¯Šæ–­å’Œé‰´åˆ«è¯Šæ–­ï¼Œä¸è¦åœ¨å›ç­”ä¸­åŒ…å«å…¶ä»–å†…å®¹ã€‚"}
#     ]
#     diagnosis_response = generate_response(messages_round1)
#     print("\nğŸ”¹ **ç¬¬ä¸€è½® (è¯Šæ–­ & é‰´åˆ«è¯Šæ–­):**\n", diagnosis_response)

#     # **ç¬¬äºŒè½®: ç”¨æˆ·è¾“å…¥äºŒè½®é—®é¢˜**
#     print("\nğŸ’¡ **è¯·æŸ¥çœ‹ä¸Šé¢çš„è¯Šæ–­ç»“æœï¼Œç°åœ¨æ‚¨å¯ä»¥è¿›ä¸€æ­¥å’¨è¯¢ã€‚**")
#     second_input = input("âœï¸ æ‚£è€…: ")
#     if second_input.lower() == "exit":
#         print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
#         return
    
#     # **ç¬¬äºŒè½®: è¯Šç–—æŒ‡å— & æ²»ç–—æ–¹æ¡ˆ**
#     messages_round2 = [
#         {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
#         {"role": "user", "content": f"æ ¹æ®ä¸Šè¿°å›ç­”ï¼Œä½ è€ƒè™‘æ ¹æ®è¿™ä½æ‚£è€…{patient_input}æè¿°çš„å†…å®¹ï¼Œè€ƒè™‘æ‚£è€…çš„è¯Šæ–­æ˜¯{diagnosis_response}ã€‚\n"
#                                     "è¿™æ˜¯è¿™ä¸ªç–¾ç—…çš„è¯Šç–—æŒ‡å—{æŒ‡å—}ï¼Œä½ å¿…é¡»æ ¹æ®æŒ‡å—ï¼Œå»å›ç­”ä»¥ä¸‹å†…å®¹:\n"
#                                     "1. æ‚£è€…ç—…æƒ…çš„åˆ†æ\n"
#                                     "2. æœ€ç»ˆè€ƒè™‘çš„è¯Šæ–­\n"
#                                     "3. è¿›ä¸€æ­¥æ£€æŸ¥åŠæ²»ç–—æ–¹æ¡ˆ\n"
#                                     "åœ¨å›ç­”è¿‡ç¨‹ä¸­ï¼Œè¯·å¼•ç”¨è¯Šç–—æŒ‡å—çš„åŸæ–‡ã€‚"}
#     ]
#     treatment_response = generate_response(messages_round2)
#     print("\nğŸ”¹ **ç¬¬äºŒè½® (ç—…æƒ…åˆ†æ & æ²»ç–—æ–¹æ¡ˆ):**\n", treatment_response)
    
#     # **ç¬¬ä¸‰è½®: ç”¨æˆ·è¾“å…¥ä¸‰è½®é—®é¢˜**
#     print("\nğŸ’¡ **è¯·è¾“å…¥æ‚¨çš„æœ€ç»ˆå’¨è¯¢é—®é¢˜ï¼Œè®©åŒ»ç”Ÿç»™æ‚¨ä¸€ä¸ªå®Œæ•´çš„æ€»ç»“ã€‚**")
#     third_input = input("âœï¸ æ‚£è€…: ")
#     if third_input.lower() == "exit":
#         print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
#         return

#     # **ç¬¬ä¸‰è½®: åŒ»ç”Ÿè§’è‰²æ€»ç»“ & è¯¦ç»†å›ç­”**
#     messages_round3 = [
#         {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
#         {"role": "user", "content": f"æ ¹æ®ä¸Šè¿°å›ç­”çš„å†…å®¹ï¼Œè¿›è¡Œæ€»ç»“ï¼Œå¹¶ä»¥åŒ»ç”Ÿçš„è§’è‰²è¯¦ç»†çš„å›ç­”æ‚£è€…ã€‚\n"
#                                     "è¯·ä½¿ç”¨ä¸“ä¸šçš„åŒ»å­¦ç”¨è¯­ï¼Œç¡®ä¿æ‚£è€…å¯ä»¥ç†è§£ï¼Œå¹¶æä¾›è¯¦ç»†çš„æ²»ç–—å»ºè®®ã€‚"}
#     ]
#     final_response = generate_response(messages_round3)
#     print("\nğŸ”¹ **ç¬¬ä¸‰è½® (åŒ»ç”Ÿè¯¦ç»†å›ç­”æ‚£è€…):**\n", final_response)

#     print("\nâœ… **é—®ç­”ç»“æŸï¼Œæ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼**")

# if __name__ == "__main__":
#     three_stage_medical_qna()
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# **LoRA è®­ç»ƒæ¨¡å‹è·¯å¾„**
LORA_MODEL_PATH = "/root/autodl-tmp/MedGuideQG/lora_medical"
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"

# **åŠ è½½ tokenizer**
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# **åŠ è½½åŸºç¡€æ¨¡å‹**
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    load_in_8bit=True,  # **å…¼å®¹ `bitsandbytes` é‡åŒ–**
    device_map="auto",
)

# **åŠ è½½ LoRA è®­ç»ƒå¥½çš„æƒé‡**
if os.path.exists(LORA_MODEL_PATH):
    model = PeftModel.from_pretrained(model, LORA_MODEL_PATH)
    print("âœ… åŠ è½½ LoRA è®­ç»ƒæ¨¡å‹æˆåŠŸï¼")
else:
    print("âš ï¸ æœªæ‰¾åˆ° LoRA è®­ç»ƒæ¨¡å‹ï¼Œä½¿ç”¨åŸå§‹æ¨¡å‹ï¼")

# **è®¾å¤‡é€‰æ‹©**
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
model.to(DEVICE)

def generate_response(user_input):
    """ ç”Ÿæˆ AI å›ç­” """
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
        {"role": "user", "content": user_input},
    ]

    # **æ ¼å¼åŒ–ä¸ºæ¨¡å‹æ‰€éœ€çš„æ ¼å¼**
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # **è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥**
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    # **ç”Ÿæˆå›ç­”**
    with torch.no_grad():
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=256,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
        )

    # **æå–ç”Ÿæˆçš„å†…å®¹**
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    # **è§£ç æ¨¡å‹è¾“å‡º**
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def interactive_medical_qna():
    """ äº¤äº’å¼ä¸‰é˜¶æ®µåŒ»ç–—é—®ç­” """
    
    print("\nğŸ©º **æ‚¨å¥½ï¼Œè¿™é‡Œæ˜¯æ™ºæ…§åŒ»ç–—å°åŠ©æ‰‹ï¼Œè¯·æ‚¨æè¿°æ‚¨çš„ç—…æƒ…ã€‚**\n")
    
    # **ç¬¬ä¸€è½®: ç”¨æˆ·è¾“å…¥ç—…æƒ…æè¿°**
    patient_input = input("ğŸ“ æ‚£è€…: ")
    if patient_input.lower() == "exit":
        print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
        return
    
    # **ç¬¬ä¸€è½®: è¯Šæ–­ & é‰´åˆ«è¯Šæ–­**
    messages_round1 = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
        {"role": "user", "content": f"ä½ å°†æ ¹æ®æ‚£è€…åœ¨{patient_input}ä¸­æä¾›çš„ä¿¡æ¯ï¼Œä»”ç»†æ€è€ƒï¼Œç»™å‡º1ä¸ªå¯èƒ½çš„è¯Šæ–­ã€‚\n"
                                    "éšåæ ¹æ®å¯èƒ½æ€§ä»é«˜åˆ°ä½ï¼Œä¾æ¬¡ç»™å‡º3è‡³5ä¸ªå¯èƒ½çš„é‰´åˆ«è¯Šæ–­ã€‚\n"
                                    "è¯·åªå›ç­”è¯Šæ–­å’Œé‰´åˆ«è¯Šæ–­ï¼Œä¸è¦åœ¨å›ç­”ä¸­åŒ…å«å…¶ä»–å†…å®¹ã€‚"}
    ]
    diagnosis_response = generate_response(patient_input)
    print("\nğŸ”¹ **ç¬¬ä¸€è½® (è¯Šæ–­ & é‰´åˆ«è¯Šæ–­):**\n", diagnosis_response)

    # **ç¬¬äºŒè½®: ç”¨æˆ·è¾“å…¥äºŒè½®é—®é¢˜**
    second_input = input("âœï¸ æ‚£è€…: ")
    if second_input.lower() == "exit":
        print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
        return
    
    # **ç¬¬äºŒè½®: è¯Šç–—æŒ‡å— & æ²»ç–—æ–¹æ¡ˆ**
    messages_round2 = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
        {"role": "user", "content": f"æ ¹æ®ä¸Šè¿°å›ç­”ï¼Œä½ è€ƒè™‘æ‚£è€…çš„è¯Šæ–­æ˜¯{diagnosis_response}ã€‚\n"
                                    "è¿™æ˜¯è¿™ä¸ªç–¾ç—…çš„è¯Šç–—æŒ‡å—{æŒ‡å—}ï¼Œè¯·æ ¹æ®æŒ‡å—å›ç­”:\n"
                                    "1. æ‚£è€…ç—…æƒ…åˆ†æ\n"
                                    "2. æœ€ç»ˆè¯Šæ–­\n"
                                    "3. è¿›ä¸€æ­¥æ£€æŸ¥åŠæ²»ç–—æ–¹æ¡ˆ\n"
                                    "è¯·å¼•ç”¨è¯Šç–—æŒ‡å—çš„åŸæ–‡ã€‚"}
    ]
    treatment_response = generate_response(second_input)
    print("\nğŸ”¹ **ç¬¬äºŒè½® (ç—…æƒ…åˆ†æ & æ²»ç–—æ–¹æ¡ˆ):**\n", treatment_response)
    
    # **ç¬¬ä¸‰è½®: ç”¨æˆ·è¾“å…¥ä¸‰è½®é—®é¢˜**
    third_input = input("âœï¸ æ‚£è€…: ")
    if third_input.lower() == "exit":
        print("ğŸ‘‹ é€€å‡ºé—®ç­”ç³»ç»Ÿã€‚")
        return

    # **ç¬¬ä¸‰è½®: åŒ»ç”Ÿè§’è‰²æ€»ç»“ & è¯¦ç»†å›ç­”**
    messages_round3 = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½åº·å¤åŒ»å­¦ä¸“å®¶"},
        {"role": "user", "content": "è¯·æ€»ç»“å¹¶ä»¥åŒ»ç”Ÿè§’è‰²å›ç­”æ‚£è€…ã€‚"}
    ]
    final_response = generate_response(third_input)
    print("\nğŸ”¹ **ç¬¬ä¸‰è½® (åŒ»ç”Ÿè¯¦ç»†å›ç­”æ‚£è€…):**\n", final_response)

    print("\nâœ… **é—®ç­”ç»“æŸï¼Œæ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼**")

if __name__ == "__main__":
    interactive_medical_qna()

